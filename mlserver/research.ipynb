{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'The video discusses the relationship between machine learning and physics, with a Nobel Prize in Physics awarded to machine learning. The speaker questions whether this is truly a physics prize, but ultimately concludes that it is fine to make exceptions for old institutions adapting to new fields.', 'key_points': ['Machine learning and neural networks are not traditional physics', 'The Nobel Prize in Physics was awarded to machine learning', 'Some argue that machine learning is more closely related to software engineering or electrical engineering than physics', 'Others see this as an exception rather than the norm'], 'topics': ['Machine Learning', 'Neural Networks', 'Nobel Prizes', 'Physics', 'Exceptional Cases'], 'key_moments': [{'timestamp': '00:10:00', 'description': 'The speaker introduces the topic of machine learning and its relationship to physics'}, {'timestamp': '00:20:00', 'description': \"The Nobel Prize in Physics is awarded to machine learning, sparking debate about whether it's truly a physics prize\"}, {'timestamp': '00:30:00', 'description': 'Some argue that machine learning is more closely related to software engineering or electrical engineering than physics'}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "# from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "from pathlib import Path\n",
    "import ollama\n",
    "\n",
    "# model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "transcript = \"\"\"\n",
    "[0.00 - 10.00]: In this video, I will explain the very basics of artificial neural networks for people who have no idea what they are and how they work.\n",
    "[10.00 - 12.00]: Like at all.\n",
    "[12.00 - 25.00]: The 2024 Nobel Prize in Physics was awarded to John Hopfield and Jeffrey Hinton for their contributions to artificial neural networks, enabling machine learning and the recent breakthroughs in AI.\n",
    "[25.00 - 27.00]: Wait what?\n",
    "[28.00 - 35.00]: Well, not gonna lie. Like most observers, I was very surprised by this choice.\n",
    "[35.00 - 41.00]: Not because I feel it's undisurbing, but because it's categorized as physics.\n",
    "[42.00 - 43.00]: I don't know.\n",
    "[43.00 - 46.00]: I'm the one who is the king of music, I'm the one who is the king of physics.\n",
    "[46.00 - 47.00]: Physics!\n",
    "[47.00 - 48.00]: Motherf**k!\n",
    "[71.00 - 75.00]: What I'm going to do here is go into a lot of details or even math.\n",
    "[75.00 - 83.00]: Because this is more of a, I don't know anything about machine learning, please explain to me in SpongeBob term situation.\n",
    "[83.00 - 85.00]: First, neural networks.\n",
    "[85.00 - 90.00]: Neural networks are based on how biological neurons or nerve cells work in our brain.\n",
    "[90.00 - 97.00]: These cells take in several input signals, process them, and generate an output signal or signals.\n",
    "[97.00 - 102.00]: Our brain is made up of a complex network of those neurons.\n",
    "[102.00 - 108.00]: And artificial neural networks are just computer programs modeled on this very structure.\n",
    "[108.00 - 112.00]: To be clear, they are not hardware. They do not exist materially.\n",
    "[112.00 - 117.00]: They are just a diagram describing the structure of a computation.\n",
    "[117.00 - 123.00]: I can draw an artificial neural network on a piece of paper and do all the calculations by hand.\n",
    "[123.00 - 127.00]: That is the exact same thing that happens inside the computer.\n",
    "[127.00 - 129.00]: Or slower of course.\n",
    "[129.00 - 131.00]: And with more errors.\n",
    "[131.00 - 133.00]: But that's the first thing.\n",
    "[133.00 - 139.00]: Artificial neural networks are just computer programs, or rather a structure describing a program.\n",
    "[139.00 - 145.00]: They can be visualized by a graph containing edges that carry inputs and or outputs,\n",
    "[145.00 - 148.00]: and notes where the computations are performed.\n",
    "[148.00 - 153.00]: Typically, notes receive many inputs from all connected nodes.\n",
    "[153.00 - 160.00]: Computing output and send this to all connected nodes again, like a biological neuron.\n",
    "[160.00 - 167.00]: A simple example would be that at a node, all incoming inputs are just added up.\n",
    "[167.00 - 176.00]: Additionally, edges are weighted, meaning they will influence the computation at a node more or less depending on their weight.\n",
    "[176.00 - 183.00]: For example, all inputs at a node might at first be multiplied by their weight and then added up.\n",
    "[183.00 - 187.00]: Notice that all the computations performed here are very basic.\n",
    "[187.00 - 190.00]: It's just addition and multiplication.\n",
    "[190.00 - 195.00]: And even with just a small number of nodes, you can have a gigantic number of edges.\n",
    "[195.00 - 201.00]: And this is where the great complexity and also the power of neural networks come from.\n",
    "[201.00 - 209.00]: Depending on a network, the number of edges can scale exponentially or even super exponentially with the number of nodes.\n",
    "[209.00 - 214.00]: You can even see that by the interesting optical patterns that such networks create.\n",
    "[214.00 - 219.00]: You can literally tell the complexity just by looking at them.\n",
    "[219.00 - 225.00]: But basically, a neural network is just a very complex, convoluted function.\n",
    "[225.00 - 232.00]: You put some numbers in at one end, some computations are performed and then you get some numbers out.\n",
    "[243.00 - 253.00]: A hotfield network consists of nodes that can only be in one of two states and are connected to all other nodes by weighted edges that go in both directions symmetrically.\n",
    "[253.00 - 257.00]: In each computational step, just a single node is chosen.\n",
    "[257.00 - 264.00]: It takes all inputs from connected nodes together with the weights attached and computes whether it should be flipped or not.\n",
    "[264.00 - 267.00]: Then another node is chosen, et cetera, et cetera.\n",
    "[267.00 - 280.00]: A hotfield network can be used to first store patterns and then when it is given another pattern as input, it can retrace to which stored pattern this is the most similar to.\n",
    "[280.00 - 285.00]: So basically, autocorrect and autocomplete for patterns.\n",
    "[285.00 - 295.00]: So for example, you could encode the pixels of an image as a pattern and then reconstruct this original pattern from inputs that are similar.\n",
    "[295.00 - 300.00]: The way this is done is based on an idea from physics, minimizing energy.\n",
    "[300.00 - 305.00]: You will have noticed that with gravity, things don't move around randomly.\n",
    "[305.00 - 310.00]: On their own, balls don't ever roll uphill, just down here.\n",
    "[310.00 - 312.00]: How does a ball know where to roll?\n",
    "[312.00 - 315.00]: It does not have to try out all possible paths.\n",
    "[315.00 - 320.00]: Gravity will always make it roll the path most directly down here.\n",
    "[320.00 - 323.00]: This way, energy is minimized.\n",
    "[323.00 - 326.00]: The same principle is applied to the hotfield network.\n",
    "[326.00 - 335.00]: The stored patterns are set as states of lowest energy, local minima, which is achieved by fixing the weights of all edges.\n",
    "[335.00 - 345.00]: Then, during each step of computation, the energy of the network pattern is evaluated and is changed such that the energies are always minimized.\n",
    "[345.00 - 349.00]: This will iterate the pattern to the closest local minimum.\n",
    "[349.00 - 351.00]: So the closest stored pattern.\n",
    "[351.00 - 362.00]: Note that this is a very basic network with a lot of limitations, but it was one of the first demonstrations of what could be achieved with neural networks.\n",
    "[374.00 - 379.00]: Let's look at a more capable neural network, classical machine learning.\n",
    "[380.00 - 391.00]: For example, let's imagine we have a neural network where you enter an image of an animal as input and it tries to figure out whether this is an image of a cat or dog.\n",
    "[391.00 - 394.00]: You start with a network like this.\n",
    "[394.00 - 402.00]: Note that there is no clear rule how large a network like this has to be, how many nodes and how many layers it needs, etc.\n",
    "[402.00 - 406.00]: All of those things come down to trial and error or experience.\n",
    "[407.00 - 412.00]: Also, the initial weights can be either random or based on experience as well.\n",
    "[412.00 - 416.00]: You have a number of nodes that take input from outside the program.\n",
    "[416.00 - 419.00]: In this case, the brightness of pixels of an image.\n",
    "[419.00 - 423.00]: So, at each input node, you now have a number.\n",
    "[423.00 - 431.00]: These numbers are then sent to the next layer of nodes, which are called hidden nodes, because they are calculating just an intermediate step of the program.\n",
    "[432.00 - 442.00]: Again, each node gets a lot of numbers as input and each number will be multiplied by its weight and all of this will then be summed up to get a new number.\n",
    "[442.00 - 451.00]: Okay, so the numbers of the input layer were the brightness of pixels, but what are the numbers calculated by the hidden layers?\n",
    "[451.00 - 457.00]: And that's kind of the thing, and that's why we often talk about AI being a black box.\n",
    "[457.00 - 464.00]: Because these numbers are just representations for some characteristic or pattern within the input.\n",
    "[464.00 - 470.00]: It's nothing a human could do anything with. In fact, to us, it looks like random noise.\n",
    "[470.00 - 473.00]: We as humans would use completely different criteria.\n",
    "[473.00 - 478.00]: For example, we would look at the size of the head or the shape of the ears.\n",
    "[478.00 - 483.00]: We would use all those to figure out whether an animal were a cat or dog.\n",
    "[483.00 - 487.00]: A neural network does not know what ears are.\n",
    "[487.00 - 492.00]: All it sees is numbers and more numbers and patterns within the numbers.\n",
    "[492.00 - 502.00]: Finally, an output is generated. In this case, a probability that the picture shows a cat and a probability that it is a dog.\n",
    "[502.00 - 509.00]: And that's where training comes in. You need to tell the network whether the result is correct or not.\n",
    "[509.00 - 516.00]: For example, the network puts out 80% probability for dog and 20% for cat.\n",
    "[516.00 - 519.00]: And the image actually shows a dog.\n",
    "[519.00 - 529.00]: Now, the weights of the network that led to the correct dog output will be amplified, while those weights that led to the wrong cat output will be weakened.\n",
    "[529.00 - 536.00]: And once a network has been through millions or even billions of those learning cycles, it will be trained.\n",
    "[536.00 - 542.00]: And it will be pretty good at figuring out whether an image shows a cat or dog.\n",
    "[542.00 - 548.00]: Or recognize handwriting or recognize speech or whatever it was trained to do.\n",
    "[548.00 - 554.00]: And again, the network I've shown is somewhat simplified and even outdated.\n",
    "[554.00 - 561.00]: But it's very good at getting across the basic ideas of machine learning, and that's why I used it.\n",
    "[561.00 - 568.00]: So physics Nobel for machine learning.\n",
    "[568.00 - 577.00]: The justification was that a lot of the basic ideas stem from physical models or physical concepts.\n",
    "[577.00 - 584.00]: And also that the machine learning methods were heavily applied in physics.\n",
    "[584.00 - 592.00]: For example, to analyze output from particle colliders or to construct images of black holes, etc.\n",
    "[592.00 - 600.00]: And all of this is true, but does that mean that artificial neural networks and machine learning is physics?\n",
    "[600.00 - 607.00]: Well, it's not really physics, but it's also not really not physics.\n",
    "[607.00 - 616.00]: And there have been Nobel Prizes in physics before that were closer to mechanical engineering or electrical engineering.\n",
    "[616.00 - 620.00]: And now we also have software engineering.\n",
    "[620.00 - 625.00]: I guess what it all comes down to, at least for me, is that it's physics.\n",
    "[625.00 - 633.00]: I guess what it all comes down to, at least for me, is that it's fine to make this kind of exceptions.\n",
    "[633.00 - 642.00]: But only as an exception, but if this kind of stuff became the norm, why call it Nobel Prizes in physics anymore?\n",
    "[642.00 - 650.00]: I get that. Institutions, especially old institutions, from time to time have to show that they're still up to date and they're still relevant.\n",
    "[650.00 - 655.00]: But you know, you also have to do what you're supposed to do.\n",
    "[655.00 - 659.00]: We don't need a second touring award. We already have one.\n",
    "[659.00 - 668.00]: Anyway, like I said in the intro, this video is focused on the very broad ideas of machine learning.\n",
    "[668.00 - 675.00]: If you're interested in much more detailed treatments, including even math, you can check out those links.\n",
    "[675.00 - 679.00]: I think these are really great videos.\n",
    "[681.00 - 688.00]: As a final thought, hands down the best thing about this was the means.\n",
    "[688.00 - 692.00]: So I'll let you enjoy a couple of those as an outro.\n",
    "[692.00 - 697.00]: See you again next year, hopefully with a little bit more physics.\n",
    "[710.00 - 713.00]: You\n",
    "[740.00 - 743.00]: You\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def analyze_transcript(transcript_path):\n",
    "    \"\"\"Analyze transcript using Ollama LLM.\"\"\"\n",
    "    with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "        transcript_text = f.read()\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "    Analyze the following transcript and provide:\n",
    "    1. A concise summary\n",
    "    2. Key points discussed\n",
    "    3. Main topics covered\n",
    "    4. Most important moments with their approximate timestamps (format: [HH:MM:SS] - description)\n",
    "    \n",
    "    Transcript:\n",
    "    {transcript_text}\n",
    "\n",
    "    Respond in the following JSON format:\n",
    "        {{\n",
    "            \"summary\": \"string\",\n",
    "            \"key_points\": [\"point1\", \"point2\", ...],\n",
    "            \"topics\": [\"topic1\", \"topic2\", ...],\n",
    "            \"key_moments\": [\n",
    "                {{\"timestamp\": \"HH:MM:SS\", \"description\": \"string\"}}\n",
    "            ]\n",
    "        }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Respond in the following JSON format:\n",
    "    # {{\n",
    "    #     \"summary\": \"string\",\n",
    "    #     \"key_points\": [\"point1\", \"point2\", ...],\n",
    "    #     \"topics\": [\"topic1\", \"topic2\", ...],\n",
    "    #     \"key_moments\": [\n",
    "    #         {{\"timestamp\": \"HH:MM:SS\", \"description\": \"string\"}}\n",
    "    #     ]\n",
    "    # }}\n",
    "    \n",
    "    # prompt = PromptTemplate(\n",
    "    #     input_variables=[\"transcript\"],\n",
    "    #     template=prompt_template\n",
    "    # )\n",
    "\n",
    "    prompt = prompt_template\n",
    "\n",
    "    # response = ollama.chat(\n",
    "    #     model='llama3.2',\n",
    "    #     messages=[{\n",
    "    #         'role': 'user',\n",
    "    #         'content': prompt,\n",
    "    #         'format': 'json'\n",
    "    #     }]\n",
    "    # )\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        format='json'  # Request JSON format explicitly\n",
    "    )\n",
    "    \n",
    "    # response = model.invoke(prompt.format(transcript=transcript_text))\n",
    "    \n",
    "    try:\n",
    "        # Parse the response to ensure it's valid JSON\n",
    "        analysis_data = json.loads(response['message']['content'])\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback structure if response isn't valid JSON\n",
    "        print('fallback')\n",
    "        analysis_data = {\n",
    "            \"summary\": response['message']['content'],\n",
    "            \"key_points\": [],\n",
    "            \"topics\": [],\n",
    "            \"key_moments\": []\n",
    "        }\n",
    "    \n",
    "    return analysis_data\n",
    "\n",
    "data = analyze_transcript('../transcript.txt')\n",
    "# data = json.loads(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'timestamp': '00:10:00',\n",
       "  'description': 'The speaker introduces the topic of machine learning and its relationship to physics'},\n",
       " {'timestamp': '00:20:00',\n",
       "  'description': \"The Nobel Prize in Physics is awarded to machine learning, sparking debate about whether it's truly a physics prize\"},\n",
       " {'timestamp': '00:30:00',\n",
       "  'description': 'Some argue that machine learning is more closely related to software engineering or electrical engineering than physics'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['key_moments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 207\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mohammed\\anaconda3\\envs\\education\\Lib\\asyncio\\runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug, loop_factory)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import whisper\n",
    "from ollama import AsyncClient\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "class MediaProcessor:\n",
    "    def __init__(self, base_dir: str = \"media_processing\"):\n",
    "        \"\"\"Initialize the MediaProcessor with necessary directories.\"\"\"\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.temp_dir = self.base_dir / \"temp\"\n",
    "        self.transcription_dir = self.base_dir / \"transcriptions\"\n",
    "        self.keyframes_dir = self.base_dir / \"keyframes\"\n",
    "        \n",
    "        # Create all necessary directories\n",
    "        for dir_path in [self.temp_dir, self.transcription_dir, self.keyframes_dir]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize Whisper model\n",
    "        self.whisper_model = whisper.load_model(\"base\")\n",
    "        \n",
    "        # Initialize Ollama AsyncClient\n",
    "        self.client = AsyncClient()\n",
    "\n",
    "    async def extract_audio(self, mp4_path: str | Path) -> Path:\n",
    "        \"\"\"Extract audio from MP4 and convert to WAV format.\"\"\"\n",
    "        mp4_path = Path(mp4_path)\n",
    "        if not mp4_path.exists():\n",
    "            raise FileNotFoundError(f\"Video file not found: {mp4_path}\")\n",
    "            \n",
    "        wav_path = self.temp_dir / f\"{mp4_path.stem}.wav\"\n",
    "        \n",
    "        command = [\n",
    "            'ffmpeg', '-i', str(mp4_path),\n",
    "            '-vn', '-acodec', 'pcm_s16le',\n",
    "            '-ar', '16000', '-ac', '1',\n",
    "            str(wav_path)\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            process = await asyncio.create_subprocess_exec(\n",
    "                *command,\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stderr=asyncio.subprocess.PIPE\n",
    "            )\n",
    "            stdout, stderr = await process.communicate()\n",
    "            \n",
    "            if process.returncode != 0:\n",
    "                raise RuntimeError(f\"FFmpeg failed: {stderr.decode()}\")\n",
    "                \n",
    "            return wav_path\n",
    "        except Exception as e:\n",
    "            if wav_path.exists():\n",
    "                wav_path.unlink()\n",
    "            raise RuntimeError(f\"Audio extraction failed: {str(e)}\")\n",
    "\n",
    "    async def transcribe_audio(self, wav_path: Path) -> Tuple[Path, Dict[str, Any]]:\n",
    "        \"\"\"Transcribe WAV file using Whisper.\"\"\"\n",
    "        if not wav_path.exists():\n",
    "            raise FileNotFoundError(f\"WAV file not found: {wav_path}\")\n",
    "            \n",
    "        try:\n",
    "            result = await asyncio.to_thread(self.whisper_model.transcribe, str(wav_path))\n",
    "            \n",
    "            transcript_path = self.transcription_dir / f\"{wav_path.stem}_transcript.txt\"\n",
    "            async with asyncio.Lock():\n",
    "                with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(result[\"text\"])\n",
    "            \n",
    "            return transcript_path, result\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Transcription failed: {str(e)}\")\n",
    "\n",
    "    async def analyze_transcript(self, transcript_path: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze transcript using Ollama AsyncClient.\"\"\"\n",
    "        if not transcript_path.exists():\n",
    "            raise FileNotFoundError(f\"Transcript file not found: {transcript_path}\")\n",
    "            \n",
    "        try:\n",
    "            async with asyncio.Lock():\n",
    "                with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                    transcript_text = f.read()\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Analyze the following transcript and provide:\n",
    "            1. A concise summary\n",
    "            2. Key points discussed\n",
    "            3. Main topics covered\n",
    "            4. Most important moments with their approximate timestamps (format: [HH:MM:SS] - description)\n",
    "            \n",
    "            Transcript:\n",
    "            {transcript_text}\n",
    "\n",
    "            Respond in the following JSON format:\n",
    "            {{\n",
    "                \"summary\": \"string\",\n",
    "                \"key_points\": [\"point1\", \"point2\", ...],\n",
    "                \"topics\": [\"topic1\", \"topic2\", ...],\n",
    "                \"key_moments\": [\n",
    "                    {{\"timestamp\": \"HH:MM:SS\", \"description\": \"string\"}}\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\"\n",
    "\n",
    "            response = await self.client.chat(\n",
    "                model='llama2',\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                format='json'  # Request JSON format explicitly\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                analysis_data = json.loads(response['message']['content'])\n",
    "            except json.JSONDecodeError:\n",
    "                analysis_data = {\n",
    "                    \"summary\": response['message']['content'],\n",
    "                    \"key_points\": [],\n",
    "                    \"topics\": [],\n",
    "                    \"key_moments\": []\n",
    "                }\n",
    "            \n",
    "            return analysis_data\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Transcript analysis failed: {str(e)}\")\n",
    "\n",
    "    async def extract_keyframes(self, mp4_path: Path, key_moments: list) -> Path:\n",
    "        \"\"\"Extract images from key moments in the MP4 file.\"\"\"\n",
    "        if not mp4_path.exists():\n",
    "            raise FileNotFoundError(f\"Video file not found: {mp4_path}\")\n",
    "            \n",
    "        base_name = mp4_path.stem\n",
    "        keyframes_subdir = self.keyframes_dir / base_name\n",
    "        keyframes_subdir.mkdir(exist_ok=True)\n",
    "        \n",
    "        processes = []\n",
    "        for moment in key_moments:\n",
    "            timestamp = moment.get('timestamp', '00:00:00')\n",
    "            description = moment.get('description', '').replace(' ', '_')[:30]\n",
    "            output_path = keyframes_subdir / f\"keyframe_{timestamp}_{description}.jpg\"\n",
    "            \n",
    "            command = [\n",
    "                'ffmpeg', '-ss', timestamp,\n",
    "                '-i', str(mp4_path),\n",
    "                '-vframes', '1',\n",
    "                '-q:v', '2',\n",
    "                '-y',\n",
    "                str(output_path)\n",
    "            ]\n",
    "            \n",
    "            process = asyncio.create_subprocess_exec(\n",
    "                *command,\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stderr=asyncio.subprocess.PIPE\n",
    "            )\n",
    "            processes.append(process)\n",
    "        \n",
    "        try:\n",
    "            await asyncio.gather(*(process for process in processes))\n",
    "            return keyframes_subdir\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Keyframe extraction failed: {str(e)}\")\n",
    "\n",
    "    async def process_media(self, mp4_path: str | Path) -> Dict[str, Any]:\n",
    "        \"\"\"Main async processing pipeline.\"\"\"\n",
    "        try:\n",
    "            wav_path = await self.extract_audio(mp4_path)\n",
    "            transcript_path, transcript_result = await self.transcribe_audio(wav_path)\n",
    "            analysis = await self.analyze_transcript(transcript_path)\n",
    "            keyframes_dir = await self.extract_keyframes(mp4_path, analysis.get('key_moments', []))\n",
    "            \n",
    "            # Cleanup\n",
    "            try:\n",
    "                if wav_path.exists():\n",
    "                    wav_path.unlink()\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to cleanup temporary file {wav_path}: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                'transcript_path': str(transcript_path),\n",
    "                'analysis': analysis,\n",
    "                'keyframes_dir': str(keyframes_dir)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Cleanup on failure\n",
    "            try:\n",
    "                if 'wav_path' in locals() and wav_path.exists():\n",
    "                    wav_path.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "            raise RuntimeError(f\"Media processing failed: {str(e)}\")\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        processor = MediaProcessor()\n",
    "        result = await processor.process_media(\"../ai.mp4\")\n",
    "        print(f\"Transcript saved at: {result['transcript_path']}\")\n",
    "        print(f\"Keyframes saved in: {result['keyframes_dir']}\")\n",
    "        print(f\"Analysis: {result['analysis']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "def analyze_transcript(transcript_path):\n",
    "    \"\"\"Analyze transcript using Ollama LLM.\"\"\"\n",
    "    # with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "    #     transcript_text = f.read()\n",
    "\n",
    "    # prompt_template = f\"\"\"\n",
    "    # Analyze the following question paper and recommend the best references among the lists:\n",
    "    # 1. A concise summary\n",
    "    # 2. Key points discussed\n",
    "    # 3. Main topics covered\n",
    "    # 4. Most important moments with their approximate timestamps (format: [HH:MM:SS] - description)\n",
    "    \n",
    "    # Transcript:\n",
    "    # {transcript}\n",
    "    \n",
    "    # Respond using JSON\n",
    "    # \"\"\"\n",
    "\n",
    "    # prompt = prompt_template\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Generate a list of recommended study materials for students preparing for their **Computer Networks** exam. Consider the following details:\n",
    "\n",
    "        - **Exam Type**: Final Exam\n",
    "        - **Syllabus Modules**:\n",
    "        - Network Models & Protocols\n",
    "        - Data Link Layer & MAC Addressing\n",
    "        - IP Addressing & Subnetting\n",
    "        - Transport Layer Protocols (TCP/UDP)\n",
    "        - Network Security Basics\n",
    "        - Network Troubleshooting Tools\n",
    "\n",
    "        Each recommendation should include:\n",
    "        - The **filename** of the reference.\n",
    "        - **Description** explaining why this material is relevant to the topic.\n",
    "        - **File type** (e.g., PDF, Video, Presentation).\n",
    "        - **Date added** (to prioritize recent materials).\n",
    "\n",
    "        Here is the list of available reference files in the classroom:\n",
    "        1. Filename: Intro_to_Networks.pdf | Description: Basic network principles and models | Filetype: PDF | Date added: 2024-09-10\n",
    "        2. Filename: Subnetting_Guide.mp4 | Description: Step-by-step guide to IP subnetting | Filetype: Video | Date added: 2024-09-20\n",
    "        3. Filename: TCP_vs_UDP.ppt | Description: Comparison of TCP and UDP protocols | Filetype: Presentation | Date added: 2024-10-01\n",
    "        4. Filename: Network_Security_101.pdf | Description: Introduction to network security basics | Filetype: PDF | Date added: 2024-10-05\n",
    "        5. Filename: Packet_Tracer_Lab.pdf | Description: Hands-on exercises for network troubleshooting using Packet Tracer | Filetype: PDF | Date added: 2024-10-12\n",
    "\n",
    "        **Output format**:\n",
    "        1. Module: [Module name, e.g., 'Network Models & Protocols']\n",
    "        - **Recommended Study Material**:\n",
    "            - Filename: [Filename]\n",
    "            - Description: [Relevance description]\n",
    "            - File type: [File type]\n",
    "            - Date added: [Date]\n",
    "\n",
    "    Focus on providing materials that align closely with the module topics and are appropriate for the final exam.\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.generate(\n",
    "        model='llama3.2',\n",
    "        prompt=prompt,\n",
    "        format='json'\n",
    "    )\n",
    "    \n",
    "    print(response)\n",
    "    # response = model.invoke(prompt.format(transcript=transcript_text))\n",
    "    \n",
    "    try:\n",
    "        # Parse the response to ensure it's valid JSON\n",
    "        analysis_data = json.loads(response['response'])\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback structure if response isn't valid JSON\n",
    "        print('fallback')\n",
    "        analysis_data = {\n",
    "            \"summary\": response['message']['content'],\n",
    "            \"key_points\": [],\n",
    "            \"topics\": [],\n",
    "            \"key_moments\": []\n",
    "        }\n",
    "    \n",
    "    return analysis_data\n",
    "\n",
    "data = analyze_transcript('../transcript.txt')\n",
    "# data = json.loads(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "education",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
